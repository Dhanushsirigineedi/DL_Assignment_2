{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11447255,"sourceType":"datasetVersion","datasetId":7171724},{"sourceId":11447814,"sourceType":"datasetVersion","datasetId":7172145}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Subset, DataLoader\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nimport math\n# Required libraries are imported","metadata":{"id":"3jThqkXDJIQ0","execution":{"iopub.status.busy":"2025-04-17T17:16:21.291198Z","iopub.execute_input":"2025-04-17T17:16:21.291461Z","iopub.status.idle":"2025-04-17T17:16:23.884775Z","shell.execute_reply.started":"2025-04-17T17:16:21.291444Z","shell.execute_reply":"2025-04-17T17:16:23.884195Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install --upgrade wandb\nimport wandb\n# import socket\n# socket.setdefaulttimeout(30)\nwandb.login(key='1d2423ec9b728fe6cc1e2c0b9a2af0e67a45183c')\n","metadata":{"id":"NCprfCfLdHvK","trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":596},"outputId":"b3f9eb38-cb52-4125-912b-db2a7806f753","execution":{"iopub.status.busy":"2025-04-17T17:16:23.885439Z","iopub.execute_input":"2025-04-17T17:16:23.885813Z","iopub.status.idle":"2025-04-17T17:16:34.954556Z","shell.execute_reply.started":"2025-04-17T17:16:23.885788Z","shell.execute_reply":"2025-04-17T17:16:34.953780Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.21.0)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.1)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m047\u001b[0m (\u001b[33mcs24m047-iitm-ac-in\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zior74QKD2ob","outputId":"c41b6904-ccab-44c3-c92c-7e60f2593ab8","trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:16:34.955426Z","iopub.execute_input":"2025-04-17T17:16:34.955856Z","iopub.status.idle":"2025-04-17T17:16:34.959311Z","shell.execute_reply.started":"2025-04-17T17:16:34.955834Z","shell.execute_reply":"2025-04-17T17:16:34.958444Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train_directory='/kaggle/input/dataset2/inaturalist_12K/train'\ntest_directory='/kaggle/input/dataset2/inaturalist_12K/val'","metadata":{"id":"1xVh41hAIgwf","trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:16:34.960061Z","iopub.execute_input":"2025-04-17T17:16:34.960241Z","iopub.status.idle":"2025-04-17T17:16:34.974824Z","shell.execute_reply.started":"2025-04-17T17:16:34.960226Z","shell.execute_reply":"2025-04-17T17:16:34.974235Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self, no_of_input_channels=3, no_of_classes=10,no_of_filters_in_base=32,size_of_filter=3,no_of_neurons=128,\n                            activation_function='sigmoid',dropout_probability=0.0,batch_normalization='no',filter_organizing_mode=0):\n        super(CNN, self).__init__()\n        self.activation_function_name = activation_function\n        self.batch_normalization = batch_normalization\n        if(filter_organizing_mode ==  0): # the filters are same\n            no_of_filters = [no_of_filters_in_base,no_of_filters_in_base,no_of_filters_in_base,no_of_filters_in_base,no_of_filters_in_base]\n        else:\n            no_of_filters = [no_of_filters_in_base,no_of_filters_in_base*2,no_of_filters_in_base*4,no_of_filters_in_base*8,no_of_filters_in_base*16] # the filters increase by *2\n        width = 0.0\n        height = 0.0\n\n        self.conv_layer1 = nn.Conv2d(in_channels=no_of_input_channels,out_channels=no_of_filters[0],kernel_size=size_of_filter, stride=1)\n        width = (256 - size_of_filter)+1 #256 --> width of image\n        height = (256 - size_of_filter)+1 #256 --> height of image\n        self.batch_norm1 = nn.BatchNorm2d(no_of_filters[0])  # batch normalizations\n        self.pool_layer1 = nn.MaxPool2d(kernel_size=size_of_filter, stride=2)\n        width = math.floor((width - size_of_filter)/2) + 1 # width  calculatios of feature map\n        height = math.floor((height -size_of_filter)/2) + 1 # hight calculations of feature map\n\n        self.conv_layer2 = nn.Conv2d( in_channels=no_of_filters[0], out_channels=no_of_filters[1], kernel_size=size_of_filter, stride=1)\n        width = ((width - size_of_filter))+1\n        height = ((height-size_of_filter))+1\n        self.batch_norm2 = nn.BatchNorm2d(no_of_filters[1])  # batch normalizations\n        self.pool_layer2 = nn.MaxPool2d(kernel_size=size_of_filter, stride=2)\n        width = math.floor((width - size_of_filter)/2) + 1 # width  calculatios of feature map\n        height = math.floor((height -size_of_filter)/2) + 1 # hight calculations of feature map\n\n        self.conv_layer3 = nn.Conv2d( in_channels=no_of_filters[1], out_channels=no_of_filters[2], kernel_size=size_of_filter, stride=1)\n        width = ((width - size_of_filter))+1\n        height = ((height-size_of_filter))+1\n        self.batch_norm3 = nn.BatchNorm2d(no_of_filters[2])  # batch normalizations\n        self.pool_layer3 = nn.MaxPool2d(kernel_size=size_of_filter, stride=2)\n        width = math.floor((width - size_of_filter)/2) + 1 # width  calculatios of feature map\n        height = math.floor((height -size_of_filter)/2) + 1 # hight calculations of feature map\n\n        self.conv_layer4 = nn.Conv2d( in_channels=no_of_filters[2], out_channels=no_of_filters[3], kernel_size=size_of_filter, stride=1)\n        width = ((width - size_of_filter))+1\n        height = ((height-size_of_filter))+1\n        self.batch_norm4 = nn.BatchNorm2d(no_of_filters[3])  # batch normalizations\n        self.pool_layer4 = nn.MaxPool2d(kernel_size=size_of_filter, stride=2)\n        width = math.floor((width - size_of_filter)/2) + 1 # width  calculatios of feature map\n        height = math.floor((height -size_of_filter)/2) + 1 # hight calculations of feature map\n\n        self.conv_layer5 = nn.Conv2d( in_channels=no_of_filters[3], out_channels=no_of_filters[4], kernel_size=size_of_filter, stride=1)\n        width = ((width - size_of_filter))+1\n        height = ((height-size_of_filter))+1\n        self.batch_norm5 = nn.BatchNorm2d(no_of_filters[4])  # batch normalizations\n        self.pool_layer5 = nn.MaxPool2d(kernel_size=size_of_filter, stride=2)\n        width = math.floor((width - size_of_filter)/2) + 1 # width  calculatios of feature map\n        height = math.floor((height -size_of_filter)/2) + 1 # hight calculations of feature map\n\n        self.dropout = nn.Dropout(p=dropout_probability) # added dropout to overcome overfitting.\n        self.full_connected1 = nn.Linear(no_of_filters[4] * width * height, no_of_neurons)\n        self.batch_norm6 = nn.BatchNorm1d(no_of_neurons)\n        self.full_connected2 = nn.Linear(no_of_neurons, no_of_classes)\n\n    def forward(self, x):\n        if(self.activation_function_name == 'relu'):\n            activation_function = F.relu\n        elif(self.activation_function_name == 'gelu'):\n            activation_function = F.gelu\n        elif(self.activation_function_name == 'silu'):\n            activation_function = F.silu\n        else:\n            activation_function = F.mish\n\n        if(self.batch_normalization == 'yes'):\n            x = activation_function(self.batch_norm1(self.conv_layer1(x)))\n        else:\n            x = activation_function(self.conv_layer1(x))\n        x = self.pool_layer1(x)\n\n        if(self.batch_normalization == 'yes'):\n            x = activation_function(self.batch_norm2(self.conv_layer2(x)))\n        else:\n            x = activation_function(self.conv_layer2(x))\n        x = self.pool_layer2(x)\n\n        if(self.batch_normalization == 'yes'):\n            x = activation_function(self.batch_norm3(self.conv_layer3(x)))\n        else:\n            x = activation_function(self.conv_layer3(x))\n        x = self.pool_layer3(x)\n\n        if(self.batch_normalization == 'yes'):\n            x = activation_function(self.batch_norm4(self.conv_layer4(x)))\n        else:\n            x = activation_function(self.conv_layer4(x))\n        x = self.pool_layer4(x)\n\n        if(self.batch_normalization == 'yes'):\n            x = activation_function(self.batch_norm5(self.conv_layer5(x)))\n        else:\n            x = activation_function(self.conv_layer5(x))\n        x = self.pool_layer5(x)\n\n        x = x.reshape(x.shape[0], -1)\n        if(self.batch_normalization == 'yes'):\n            x = activation_function(self.batch_norm6(self.full_connected1(x)))\n        else:\n            x = activation_function(self.full_connected1(x))\n        x = self.dropout(x)\n        x = self.full_connected2(x)\n        return x","metadata":{"id":"FsGQWkx6JNw4","execution":{"iopub.status.busy":"2025-04-17T17:16:34.975494Z","iopub.execute_input":"2025-04-17T17:16:34.975681Z","iopub.status.idle":"2025-04-17T17:16:34.992349Z","shell.execute_reply.started":"2025-04-17T17:16:34.975666Z","shell.execute_reply":"2025-04-17T17:16:34.991687Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"82Wbat2GLjXW","outputId":"32654a5e-f24b-4f19-e455-18fb5c277aba","trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:16:34.994439Z","iopub.execute_input":"2025-04-17T17:16:34.994891Z","iopub.status.idle":"2025-04-17T17:16:35.034635Z","shell.execute_reply.started":"2025-04-17T17:16:34.994874Z","shell.execute_reply":"2025-04-17T17:16:35.033935Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"transform_basic = transforms.Compose([\n    transforms.Resize((256,256)), # resized to a threshold value so that all images have same shape and size\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,),(0.5,))]) # normalized for better accuracy.\n\ntrain_dataset = datasets.ImageFolder(root=train_directory,transform=transform_basic) # train_data loading\ntraining_dataset,validation_dataset = torch.utils.data.random_split(train_dataset,[8000,1999]) #splitting the data into 80%(training) and 20%(validation) The overall data size is 9999\n\ntransform_augmented = transforms.Compose([\n    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n    transforms.RandomRotation(10),      # Randomly rotate the image by a maximum of 10 degrees\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Adjust brightness, contrast, saturation, and hue\n    transforms.RandomResizedCrop(256),  # Randomly crop and resize the image to 256x256\n    transforms.ToTensor(),              # Convert the image to a PyTorch tensor\n    transforms.Normalize((0.5,),(0.5,))  # Normalize the image\n]) # for augumenting the training data\ntrain_dataset2 = datasets.ImageFolder(root=train_directory,transform=transform_augmented)\ntraining_dataset_aug,validation_dataset_aug = torch.utils.data.random_split(train_dataset2,[8000,1999]) #  #splitting the data into 80%(training) and 20%(validation) The overall data size is 9999\n\ntest_dataset = datasets.ImageFolder(root=test_directory,transform=transform_basic); # test data loading.","metadata":{"id":"s8RAqFPBLkF-","trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:16:35.035323Z","iopub.execute_input":"2025-04-17T17:16:35.035507Z","iopub.status.idle":"2025-04-17T17:16:37.083425Z","shell.execute_reply.started":"2025-04-17T17:16:35.035481Z","shell.execute_reply":"2025-04-17T17:16:37.082911Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def data_loader_creator(augmentation_flag,batch_size): # function to return the data loaders depending on augumentation.\n    if(augmentation_flag == 'no'):\n        train_loader = torch.utils.data.DataLoader(training_dataset,batch_size =batch_size,shuffle = True,num_workers=2,pin_memory=True)\n        val_loader = torch.utils.data.DataLoader(validation_dataset,batch_size =batch_size,shuffle = True,num_workers=2,pin_memory=True)\n        return train_loader,val_loader\n    else:\n        train_loader_aug = torch.utils.data.DataLoader(training_dataset_aug,batch_size =batch_size,shuffle = True,num_workers=4,pin_memory=True)\n        val_loader_aug = torch.utils.data.DataLoader(validation_dataset_aug,batch_size =batch_size,shuffle = True,num_workers=4,pin_memory=True)\n        return train_loader_aug,val_loader_aug","metadata":{"id":"xwvPIxbbQwJo","trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:16:37.084113Z","iopub.execute_input":"2025-04-17T17:16:37.084327Z","iopub.status.idle":"2025-04-17T17:16:37.089740Z","shell.execute_reply.started":"2025-04-17T17:16:37.084301Z","shell.execute_reply":"2025-04-17T17:16:37.089028Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def Accuracy_calculator(loader,model,criterion,batch_size): # function to clculate the accuracy and loss\n    no_of_correct_predictions = 0\n    no_of_samples = 0\n    total_loss = 0.0\n    model.eval()\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device)\n            y = y.to(device=device)\n            scores = model(x)\n            loss = criterion(scores, y)\n            total_loss += loss.item()*batch_size # sum of cross entropies\n            _, predictions = scores.max(1)\n            no_of_correct_predictions += (predictions == y).sum().item() # correctly classified data\n            no_of_samples += predictions.size(0)\n    model.train()\n    return (no_of_correct_predictions / no_of_samples)*100 , total_loss","metadata":{"id":"Yq0mYvsqWZp0","trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:16:37.090488Z","iopub.execute_input":"2025-04-17T17:16:37.090757Z","iopub.status.idle":"2025-04-17T17:16:37.108320Z","shell.execute_reply.started":"2025-04-17T17:16:37.090706Z","shell.execute_reply":"2025-04-17T17:16:37.107773Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def train_the_model(no_of_neurons,no_of_filters,size_of_filter,activation_function_name,optimizer_name,batch_size,\n                    dropout_probability,no_of_epochs,learning_rate,batch_normalization,augmentation_flag,filter_organizing_mode):\n\n    train_loader,val_loader = data_loader_creator(augmentation_flag,batch_size)  # getting dataloaders.\n\n    #test_loader = torch.utils.data.DataLoader(test_data,batch_size =batchSize,shuffle = True,num_workers=2,pin_memory=True)\n\n    no_of_input_channels=3\n    no_of_classes=10\n\n    model=CNN(no_of_input_channels, no_of_classes,no_of_filters,size_of_filter,no_of_neurons,\n              activation_function_name,dropout_probability,batch_normalization,filter_organizing_mode).to(device)\n\n    if(optimizer_name == 'sgd'):\n        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n    elif(optimizer_name == 'adam'):\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    else:\n      optimizer = optim.NAdam(model.parameters(), lr=learning_rate) # optimzers selection\n\n    criterion = nn.CrossEntropyLoss() # since it is classification problem corss entropy loss is used.\n\n    for epoch in range(no_of_epochs): # performs the training.\n        for batchId, (input_images, target_classes) in enumerate(tqdm(train_loader)):\n            # Get data to cuda if possible\n            input_images = input_images.to(device=device)\n            target_classes = target_classes.to(device=device)\n            # forward\n            scores = model(input_images) # give the last layer pre-activation values.\n            loss = criterion(scores,target_classes) # gets the overll cross entropy loss for each batch\n            optimizer.zero_grad() # gradients are made to zero for each batch.\n            loss.backward()  # calculaing the gradients\n            optimizer.step() #updates the parameters\n        training_accuracy,training_loss = Accuracy_calculator(train_loader, model,criterion,batch_size) # calculates the accuracy and loss at one go\n        validation_accuracy,validation_loss = Accuracy_calculator(val_loader, model,criterion,batch_size)\n        #  the below line can be uncommenteed for test accuracy and loss\n        #test_accuracy,test_loss = check_accuracy(test_loader, model,criterion,batchSize)\n        print(f\"training_accuracy:{training_accuracy:.4f},training_loss:{training_loss:.4f}\")\n        print(f\"validation_accuracy:{validation_accuracy:.4f},validation_loss:{validation_loss:.4f}\")\n        #print(f\"test_accuracy:{test_accuracy:.4f},test_loss:{test_loss:.4f}\")\n        wandb.log({'training_accuracy':training_accuracy}) # plotting  the data in wandb\n        wandb.log({'training_loss':training_loss})\n        wandb.log({'validation_accuracy':validation_accuracy})\n        wandb.log({'validation_loss':validation_loss})","metadata":{"id":"tjVu69UXSi6X","trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:16:37.109097Z","iopub.execute_input":"2025-04-17T17:16:37.109281Z","iopub.status.idle":"2025-04-17T17:16:37.122369Z","shell.execute_reply.started":"2025-04-17T17:16:37.109267Z","shell.execute_reply":"2025-04-17T17:16:37.121674Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Sweep config for wandb plotting\n# wandb.init(project ='DA6401_Assignment_2')\nsweep_config = {\n    'name'  : \"test_run\",\n    'method': 'bayes',\n    'metric': {\n      'name': 'validation_accuracy',\n      'goal': 'maximize'\n    },\n    'parameters': {\n        'no_of_neurons': {\n            'values': [128, 256, 512]\n        },\n        'no_of_filters': {\n            'values': [32]# [[64,128,256,512, 1024], [32,32,32,32,32],[32,64,64,128,128],[128,128,64,64,32],[32,64,128,256,512]]\n        },\n        'size_of_filter': {\n            'values': [3,5]\n        },\n        'activation_function_name': {\n            'values': ['relu','gelu','silu','mish']\n        },\n        'optimizer_name': {\n            'values': ['nadam', 'adam']\n        },\n        'batch_size': {\n            'values': [32, 64,128]\n        },\n        'dropout_probability': {\n            'values': [0, 0.2, 0.4]\n        },\n        'no_of_epochs': {\n            'values': [5,10]\n        },\n        'learning_rate': {\n            'values': [1e-3, 1e-4]\n        },\n        'batch_normalization': {\n            'values': ['yes','no']\n        },\n        'augmentation_flag': {\n            'values': ['yes','no']\n        },\n        'filter_organizing_mode': {\n            'values': [0, 1]\n        }\n    }\n}\n\n# sweep_id = wandb.sweep(sweep_config, project=\"DA6401_Assignment_2\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aFev94lk6P5v","outputId":"9186cd5b-9628-4708-a8d5-e979d37d1598","trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:16:37.123081Z","iopub.execute_input":"2025-04-17T17:16:37.123239Z","iopub.status.idle":"2025-04-17T17:16:37.138673Z","shell.execute_reply.started":"2025-04-17T17:16:37.123223Z","shell.execute_reply":"2025-04-17T17:16:37.138090Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def run_experiment():\n    try:\n        run = wandb.init()  # No config argument here\n        cfg = run.config\n        run.name = (\n            f\"No_of_neurons: {cfg.no_of_neurons}, \"\n            f\"No_of_filters: {cfg.no_of_filters}, \"\n            f\"Size_of_filter: {cfg.size_of_filter}, \"\n            f\"Activation_function: {cfg.activation_function_name}, \"\n            f\"Optimizer: {cfg.optimizer_name}, \"\n            f\"Batch_size: {cfg.batch_size}, \"\n            f\"Dropout: {cfg.dropout_probability}, \"\n            f\"No_of_epochs: {cfg.no_of_epochs}, \"\n            f\"Learning_Rate: {cfg.learning_rate}, \"\n            f\"Batch_normalization: {cfg.batch_normalization}, \"\n            f\"Augmentation_flag: {cfg.augmentation_flag}, \"\n            f\"Filter_organizing_mode: {cfg.filter_organizing_mode}\"\n        )\n        train_the_model(\n            cfg.no_of_neurons,\n            cfg.no_of_filters,\n            cfg.size_of_filter,\n            cfg.activation_function_name,\n            cfg.optimizer_name,\n            cfg.batch_size,\n            cfg.dropout_probability,\n            cfg.no_of_epochs,\n            cfg.learning_rate,\n            cfg.batch_normalization,\n            cfg.augmentation_flag,\n            cfg.filter_organizing_mode,\n        )\n    except Exception as e:\n        print(f\"Error during training: {e}\")\n        if wandb.run:\n            wandb.finish(exit_code=1)\n        raise\n    finally:\n        if wandb.run:\n            wandb.finish\nif __name__==\"__main__\":\n    sweep_id = wandb.sweep(sweep_config, project=\"DA6401_Assignment_2\")\n    wandb.agent(sweep_id, run_experiment ,  count=1)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"eWap7rHM6S3v","outputId":"2465c9f6-7d5d-437d-ba0b-655d9c8e13a0","trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:16:37.139318Z","iopub.execute_input":"2025-04-17T17:16:37.139514Z","iopub.status.idle":"2025-04-17T17:26:57.075138Z","shell.execute_reply.started":"2025-04-17T17:16:37.139500Z","shell.execute_reply":"2025-04-17T17:26:57.074519Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: e0xyormb\nSweep URL: https://wandb.ai/cs24m047-iitm-ac-in/DA6401_Assignment_2/sweeps/e0xyormb\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y8868hj8 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_function_name: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \taugmentation_flag: no\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_normalization: yes\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_probability: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organizing_mode: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tno_of_epochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tno_of_filters: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tno_of_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_name: nadam\n\u001b[34m\u001b[1mwandb\u001b[0m: \tsize_of_filter: 5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250417_171645-y8868hj8</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m047-iitm-ac-in/DA6401_Assignment_2/runs/y8868hj8' target=\"_blank\">sleek-sweep-1</a></strong> to <a href='https://wandb.ai/cs24m047-iitm-ac-in/DA6401_Assignment_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs24m047-iitm-ac-in/DA6401_Assignment_2/sweeps/e0xyormb' target=\"_blank\">https://wandb.ai/cs24m047-iitm-ac-in/DA6401_Assignment_2/sweeps/e0xyormb</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m047-iitm-ac-in/DA6401_Assignment_2' target=\"_blank\">https://wandb.ai/cs24m047-iitm-ac-in/DA6401_Assignment_2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs24m047-iitm-ac-in/DA6401_Assignment_2/sweeps/e0xyormb' target=\"_blank\">https://wandb.ai/cs24m047-iitm-ac-in/DA6401_Assignment_2/sweeps/e0xyormb</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m047-iitm-ac-in/DA6401_Assignment_2/runs/y8868hj8' target=\"_blank\">https://wandb.ai/cs24m047-iitm-ac-in/DA6401_Assignment_2/runs/y8868hj8</a>"},"metadata":{}},{"name":"stderr","text":"100%|██████████| 250/250 [01:20<00:00,  3.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"training_accuracy:30.4625,training_loss:15718.4481\nvalidation_accuracy:28.8644,validation_loss:4060.5585\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 250/250 [00:51<00:00,  4.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"training_accuracy:34.2375,training_loss:15030.3058\nvalidation_accuracy:32.6163,validation_loss:3906.6484\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 250/250 [00:51<00:00,  4.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"training_accuracy:38.9750,training_loss:14210.3907\nvalidation_accuracy:33.5168,validation_loss:3780.1595\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 250/250 [00:49<00:00,  5.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"training_accuracy:38.2750,training_loss:14075.8842\nvalidation_accuracy:32.5663,validation_loss:3790.2803\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 250/250 [00:50<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"training_accuracy:42.1625,training_loss:13343.6660\nvalidation_accuracy:35.1176,validation_loss:3723.3995\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>training_accuracy</td><td>▁▃▆▆█</td></tr><tr><td>training_loss</td><td>█▆▄▃▁</td></tr><tr><td>validation_accuracy</td><td>▁▅▆▅█</td></tr><tr><td>validation_loss</td><td>█▅▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>training_accuracy</td><td>42.1625</td></tr><tr><td>training_loss</td><td>13343.66605</td></tr><tr><td>validation_accuracy</td><td>35.11756</td></tr><tr><td>validation_loss</td><td>3723.39954</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">No_of_neurons: 256, No_of_filters: 32, Size_of_filter: 5, Activation_function: mish, Optimizer: nadam, Batch_size: 32, Dropout: 0, No_of_epochs: 5, Learning_Rate: 0.0001, Batch_normalization: yes, Augmentation_flag: no, Filter_organizing_mode: 0</strong> at: <a href='https://wandb.ai/cs24m047-iitm-ac-in/DA6401_Assignment_2/runs/y8868hj8' target=\"_blank\">https://wandb.ai/cs24m047-iitm-ac-in/DA6401_Assignment_2/runs/y8868hj8</a><br> View project at: <a href='https://wandb.ai/cs24m047-iitm-ac-in/DA6401_Assignment_2' target=\"_blank\">https://wandb.ai/cs24m047-iitm-ac-in/DA6401_Assignment_2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250417_171645-y8868hj8/logs</code>"},"metadata":{}}],"execution_count":12}]}